---
title: "Twitter term-document-matrix"
author: "Hagai Levi"
date: "9 April 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Based on http://www.rdatamining.com/examples/text-mining

```{r, results='hide'}
setwd('/Users/hagai_lvi/tmp/data_scientist/assignment_3')
source('./credentials.R')
```

```{r, results='hide'}
library(twitteR)

# set twitter credentials
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_token_secret)
```

Now, we gather tweets of the user BarackObama
```{r, echo=TRUE}
maxID <- NULL
n_tweets <- 0
N_TWEETS <- 300
tweets <- NULL
while(n_tweets < N_TWEETS) {
  tmp <- userTimeline("BarackObama", n=200, maxID = maxID)
  maxID <- tmp[[length(tmp)]]$id  
  tweets <- append(tweets, tmp)
  n_tweets <- length(tweets)
}
```

Extract a corpus and a TermDocumentMatrix
```{r}
# This function returns the most frequent terms in a TermDocumentMatrix
# as a named vector that includes the frequencies
getMostFrequentTerms <- function(dtm, N){
  m <- as.matrix(dtm)
  v <- sort(rowSums(m), decreasing=TRUE)
  head(v, N)
}

# Create a dataframe from the data
df <- do.call("rbind", lapply(tweets, as.data.frame))

cat(sprintf("Matrix dimensions: %i cols, %i rows", ncol(df), nrow(df)))
library(tm)


# build a corpus, which is a collection of text documents
# VectorSource specifies that the source is character vectors.
myCorpus <- Corpus(VectorSource(df$text))

# all lowercase
myCorpus <- tm_map(myCorpus, content_transformer(tolower))

# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)

# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)

# remove stopwords
myStopwords <- c(stopwords('english'), "available", "via")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

# stem words in a text document with the snowball stemmers,
# which requires packages Snowball, RWeka, rJava, RWekajars
myCorpus <- tm_map(myCorpus, stemDocument)

# Create a TermDocumentMatrix
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
inspect(myDtm[20:30,20:30])
```

Display words associated with obama and american
```{r}
# which words are associated with "obama"?
findAssocs(myDtm, 'obama', 0.30)

# which words are associated with "american"?
findAssocs(myDtm, 'american', 0.30)

```

Convert to a term matrix
```{r}

# Transform Data into an Adjacency Matrix
termDocMatrix <- as.matrix(myDtm)

# change it to a Boolean matrix
termDocMatrix[termDocMatrix>=1] <- 1

# transform into a term-term adjacency matrix
termMatrix <- termDocMatrix %*% t(termDocMatrix)

# inspect terms numbered 5 to 10
termMatrix[15:20,15:20]
```

The corpus is large, and we can't see clearly a large graph so we are using only a few
frequent items.
```{r}
library(igraph)

# Get the most frequent words
frequent <- getMostFrequentTerms(myDtm, 20)
frequentTermMatrix <- termMatrix[names(frequent),names(frequent)]

# make a binary matrix
frequentTermMatrix[frequentTermMatrix>1] <- 1
g <- graph.adjacency(frequentTermMatrix, mode = "undirected")

# remove self loops
g <- simplify(g)
V(g)$degree <- degree(g)

# Plot the Graph
# set seed to make the layout reproducible
set.seed(100)
lay <- layout.kamada.kawai(g)
plot(g, layout=lay)

# Now add clustering to the graph
community <- walktrap.community(g)
plot(g, layout=lay, vertex.size=5, vertex.color=community$membership, asp=FALSE)

```

We got `r dim(sizes(community))` communities. The size of each community:

```{r}
sizes(community)
```

The modularity we got:
```{r}
modularity(community)
```

We want to show the betweenness of each term: 
```{r}
t(betweenness(g))
```

And show it on the graph, together with the clusters:
```{r}
plot(g, layout=lay, vertex.color=community$membership, vertex.size=betweenness(g), asp=FALSE)
```

Same goes for closeness:
```{r}
t(closeness(g))
plot(g, layout=lay, vertex.color=community$membership, vertex.size=closeness(g), asp=FALSE)
```

And as closeness generates small values, we will increases the vertexes size:
```{r}
g.closeness <- closeness(g)
g.closeness.normalized <- 10 * (g.closeness / min(g.closeness))
t(g.closeness.normalized)
plot(g, layout=lay, vertex.color=community$membership, vertex.size=g.closeness.normalized, asp=FALSE)
```

Same goes for eigen values:  
**For eigenvalues, we will not show a graph, due to negative values.**
```{r}
eigen_values <- eigen(frequentTermMatrix)$values
names(eigen_values) <- colnames(frequentTermMatrix)
eigen_values
```