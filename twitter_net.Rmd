---
title: "Twitter term-document-matrix"
author: "Hagai Levi"
date: "9 April 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

A preface to this work can be found in the [README](README.md).  

We have decided to first show clustering and only then to show the cenrality merics
so that we can include the clusters in the graph when plotting the cenrality merics,
even though it is not the order of the tasks in the assignment.  

We have based on http://www.rdatamining.com/examples/text-mining for retrieving the tweets.

```{r, results='hide'}
setwd('/Users/hagai_lvi/tmp/data_scientist/assignment_3')
source('./credentials.R')
```

```{r, results='hide'}
library(twitteR)

# set twitter credentials
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_token_secret)
```

## Collecting data
The data that we will be using is tweets by Barack Obama.  
```{r, echo=TRUE}
maxID <- NULL
n_tweets <- 0
N_TWEETS <- 300
tweets <- NULL
while(n_tweets < N_TWEETS) {
  tmp <- userTimeline("BarackObama", n=200, maxID = maxID)
  maxID <- tmp[[length(tmp)]]$id  
  tweets <- append(tweets, tmp)
  n_tweets <- length(tweets)
}
```

## Basic exploration of the data
Extract a corpus and a TermDocumentMatrix:
```{r, message=FALSE}
# This function returns the most frequent terms in a TermDocumentMatrix
# as a named vector that includes the frequencies
getMostFrequentTerms <- function(dtm, N){
  m <- as.matrix(dtm)
  v <- sort(rowSums(m), decreasing=TRUE)
  head(v, N)
}

# Create a dataframe from the data
df <- do.call("rbind", lapply(tweets, as.data.frame))

cat(sprintf("Matrix dimensions: %i cols, %i rows", ncol(df), nrow(df)))
library(tm)


# build a corpus, which is a collection of text documents
# VectorSource specifies that the source is character vectors.
myCorpus <- Corpus(VectorSource(df$text))

# all lowercase
myCorpus <- tm_map(myCorpus, content_transformer(tolower))

# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)

# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)

# remove stopwords
myStopwords <- c(stopwords('english'), "available", "via")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

# stem words in a text document with the snowball stemmers,
# which requires packages Snowball, RWeka, rJava, RWekajars
myCorpus <- tm_map(myCorpus, stemDocument)

# Create a TermDocumentMatrix
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
inspect(myDtm[20:30,20:30])
```

Display words associated with obama and american
```{r}
# which words are associated with "obama"?
findAssocs(myDtm, 'obama', 0.30)

# which words are associated with "american"?
findAssocs(myDtm, 'american', 0.30)

```

Convert to a term matrix
```{r}

# Transform Data into an Adjacency Matrix
termDocMatrix <- as.matrix(myDtm)

# change it to a Boolean matrix
termDocMatrix[termDocMatrix>=1] <- 1

# transform into a term-term adjacency matrix
termMatrix <- termDocMatrix %*% t(termDocMatrix)

# inspect terms numbered 5 to 10
termMatrix[15:20,15:20]
```

The corpus is large, and we can't see clearly a large graph so we are using only a few
frequent items.
```{r, message=FALSE}
library(igraph)

# Get the most frequent words
frequent <- getMostFrequentTerms(myDtm, 20)
frequentTermMatrix <- termMatrix[names(frequent),names(frequent)]
```

## Basic graphs
We construct a graph from the most frequent terms:
```{r}
# make a binary matrix
frequentTermMatrix[frequentTermMatrix>1] <- 1
g <- graph.adjacency(frequentTermMatrix, mode = "undirected")

# remove self loops
g <- simplify(g)
V(g)$degree <- degree(g)

# Plot the Graph
# set seed to make the layout reproducible
set.seed(100)
lay <- layout.kamada.kawai(g)
plot(g, layout=lay)
```

## Clustering

### `walktrap` clustering
Adding clustering using the `walktrap` strategy:
```{r}
# Now add clustering to the graph
community <- walktrap.community(g)
plot(g, layout=lay, vertex.size=5, vertex.color=community$membership, asp=FALSE)

```

We got `r dim(sizes(community))` communities. The size of each community:

```{r}
sizes(community)
```

The modularity we got:
```{r}
modularity(community)
```

### `edge betweenness` clustering

Now clustering using the `edge betweenness` strategy:
```{r}
# Now add clustering to the graph
community <- edge.betweenness.community(g)
plot(g, layout=lay, vertex.size=5, vertex.color=community$membership, asp=FALSE)

```

We got `r dim(sizes(community))` communities. The size of each community:

```{r}
sizes(community)
```

The modularity of `edge betweenness`:
```{r}
modularity(community)
```

## Centrality metrics

### Betweenness
We want to show the betweenness of each term: 
```{r}
t(betweenness(g))
```

And plot it on the graph, together with the clusters:
```{r}
plot(g, layout=lay, vertex.color=community$membership, vertex.size=betweenness(g), asp=FALSE)
```

### Closeness
```{r}
t(closeness(g))
plot(g, layout=lay, vertex.color=community$membership, vertex.size=closeness(g), asp=FALSE)
```

And as closeness generates small values, we will increases the vertexes size:
```{r}
g.closeness <- closeness(g)
g.closeness.normalized <- 10 * (g.closeness / min(g.closeness))
t(g.closeness.normalized)
plot(g, layout=lay, vertex.color=community$membership, vertex.size=g.closeness.normalized, asp=FALSE)
```

### Eigen values

**For eigenvalues, we will not show a graph, due to negative values.**

```{r}
eigen_values <- eigen(frequentTermMatrix)$values
names(eigen_values) <- colnames(frequentTermMatrix)
eigen_values
```